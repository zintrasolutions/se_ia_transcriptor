# Mod√®les Ollama Support√©s

Privacy IA Local Transcriptor utilise Ollama pour la traduction locale. Voici les mod√®les recommand√©s et comment les configurer.

## üöÄ Mod√®les Recommand√©s

### 1. Llama 2 (Recommand√©)
```bash
ollama pull llama3.1
```
- **Taille** : ~3.8GB
- **Performance** : Excellente pour la traduction
- **Langues** : Multilingue
- **Configuration** : `OLLAMA_MODEL=llama3.1`

### 2. Llama 2 7B Chat
```bash
ollama pull llama3.1:7b-chat
```
- **Taille** : ~4GB
- **Performance** : Optimis√© pour les conversations
- **Langues** : Multilingue
- **Configuration** : `OLLAMA_MODEL=llama3.1:7b-chat`

### 3. Mistral 7B
```bash
ollama pull mistral
```
- **Taille** : ~4.1GB
- **Performance** : Tr√®s bonne pour la traduction
- **Langues** : Multilingue
- **Configuration** : `OLLAMA_MODEL=mistral`

### 4. Code Llama
```bash
ollama pull codellama
```
- **Taille** : ~3.8GB
- **Performance** : Sp√©cialis√© pour le code, mais bon pour la traduction
- **Langues** : Multilingue
- **Configuration** : `OLLAMA_MODEL=codellama`

## üîß Configuration

### 1. Modifier le fichier .env
```env
OLLAMA_MODEL=llama3.1
OLLAMA_BASE_URL=http://localhost:11434
```

### 2. Modifier directement dans server.js
```javascript
const OLLAMA_MODEL = 'llama3.1'; // Changez ici
```

## üìä Comparaison des Mod√®les

| Mod√®le | Taille | Vitesse | Qualit√© | Recommandation |
|--------|--------|---------|---------|----------------|
| llama3.1 | 3.8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Recommand√© |
| llama3.1:7b-chat | 4GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Recommand√© |
| mistral | 4.1GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Recommand√© |
| codellama | 3.8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Sp√©cialis√© |
| llama3.1:13b | 7.3GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üíæ Plus lent |
| llama3.1:70b | 39GB | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üêå Tr√®s lent |

## üéØ Optimisations

### Pour de meilleures performances :
1. **Utilisez un GPU** si disponible
2. **Choisissez un mod√®le plus petit** pour la vitesse
3. **Ajustez les param√®tres** dans le code

### Param√®tres de traduction personnalis√©s :
```javascript
// Dans server.js, modifiez le prompt
const prompt = `Traduis le texte suivant de ${sourceLanguage} vers ${targetLanguage}. 
Garde le m√™me style et ton. Retourne seulement la traduction sans explications:

"${segment.text}"`;
```

## üîç D√©pannage

### Mod√®le non trouv√©
```bash
# Lister les mod√®les disponibles
ollama list

# T√©l√©charger un mod√®le
ollama pull llama3.1

# V√©rifier l'espace disque
df -h
```

### Performance lente
- Utilisez un mod√®le plus petit
- V√©rifiez l'utilisation CPU/GPU
- Fermez les autres applications

### Erreurs de traduction
- V√©rifiez que le mod√®le supporte les langues
- Essayez un mod√®le diff√©rent
- V√©rifiez la qualit√© du texte source

## üìù Notes

- Les mod√®les plus grands donnent de meilleures traductions mais sont plus lents
- Les mod√®les sp√©cialis√©s (comme Code Llama) peuvent √™tre moins bons pour la traduction g√©n√©rale
- Llama 2 est g√©n√©ralement le meilleur compromis qualit√©/vitesse
- Mistral est excellent pour les langues europ√©ennes
